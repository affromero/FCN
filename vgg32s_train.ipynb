{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Convolutional Network - Semantic Segmentation\n",
    "\n",
    "![image.png](imgs/1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import pytz\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "configurations = {\n",
    "    # same configuration as original work\n",
    "    # https://github.com/shelhamer/fcn.berkeleyvision.org\n",
    "    1: dict(\n",
    "        max_iteration=100000,\n",
    "        lr=1.0e-10,\n",
    "        momentum=0.99,\n",
    "        weight_decay=0.0005,\n",
    "        interval_validate=4000,\n",
    "    )\n",
    "}\n",
    "\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = configurations[1]\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_log_dir\n",
    "out = get_log_dir('vgg32s', 1, cfg)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu)\n",
    "cuda = torch.cuda.is_available()\n",
    "print('Cuda: {}'.format(cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PascalVOC Dataset - Downloaded on _`root`_ variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data/Pascal_VOC'\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import Pascal_Data\n",
    "kwargs = {'num_workers': 4} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        Pascal_Data(root, image_set='train', backbone='vgg'),\n",
    "        batch_size=1, shuffle=True, **kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        Pascal_Data(root, image_set='val', backbone='vgg'),\n",
    "        batch_size=1, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "for data, target in train_loader: break\n",
    "print(data.shape)\n",
    "print(target.shape)\n",
    "data.min()\n",
    "data_show, label_show = train_loader.dataset.untransform(data[0].cpu().clone(), target[0].cpu().clone())\n",
    "\n",
    "plt.imshow(data_show)\n",
    "plt.show()\n",
    "\n",
    "def imshow_label(label_show):\n",
    "    import matplotlib\n",
    "    import numpy as np\n",
    "    cmap = plt.cm.jet\n",
    "    # extract all colors from the .jet map\n",
    "    cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "    cmaplist[0] = (0.0,0.0,0.0,1.0)\n",
    "    cmap = cmap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "    # define the bins and normalize\n",
    "    bounds = np.arange(0,len(train_loader.dataset.class_names))\n",
    "    norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    plt.imshow(label_show, cmap=cmap, norm=norm)\n",
    "    cbar = plt.colorbar(ticks=bounds)\n",
    "    cbar.ax.set_yticklabels(train_loader.dataset.class_names)\n",
    "    plt.show()    \n",
    "    \n",
    "imshow_label(label_show)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class FCN32s(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class=21):\n",
    "        super(FCN32s, self).__init__()\n",
    "        # conv1\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=100)\n",
    "        self.relu1_1 = nn.ReLU(inplace=True)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.relu1_2 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/2\n",
    "\n",
    "        # conv2\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/4\n",
    "\n",
    "        # conv3\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.relu3_3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/8\n",
    "\n",
    "        # conv4\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu4_2 = nn.ReLU(inplace=True)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu4_3 = nn.ReLU(inplace=True)\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/16\n",
    "\n",
    "        # conv5\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_2 = nn.ReLU(inplace=True)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_3 = nn.ReLU(inplace=True)\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/32\n",
    "\n",
    "        # fc6\n",
    "        self.fc6 = nn.Conv2d(512, 4096, 7)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.drop6 = nn.Dropout2d()\n",
    "\n",
    "        # fc7\n",
    "        self.fc7 = nn.Conv2d(4096, 4096, 1)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.drop7 = nn.Dropout2d()\n",
    "\n",
    "        self.score_fr = nn.Conv2d(4096, n_class, 1)\n",
    "        self.upscore = nn.ConvTranspose2d(n_class, n_class, 64, stride=32,\n",
    "                                          bias=False)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.zero_()\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                assert m.kernel_size[0] == m.kernel_size[1]\n",
    "                initial_weight = get_upsampling_weight(\n",
    "                    m.in_channels, m.out_channels, m.kernel_size[0])\n",
    "                m.weight.data.copy_(initial_weight)\n",
    "\n",
    "    def forward(self, x, debug = False):\n",
    "        h = x\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.relu1_1(self.conv1_1(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.relu1_2(self.conv1_2(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.pool1(h)\n",
    "        if debug: print(h.data.shape)\n",
    "\n",
    "        h = self.relu2_1(self.conv2_1(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.relu2_2(self.conv2_2(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.pool2(h)\n",
    "        if debug: print(h.data.shape)\n",
    "\n",
    "        h = self.relu3_1(self.conv3_1(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.relu3_2(self.conv3_2(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.relu3_3(self.conv3_3(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.pool3(h)\n",
    "        if debug: print(h.data.shape)\n",
    "\n",
    "        h = self.relu4_1(self.conv4_1(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.relu4_2(self.conv4_2(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.relu4_3(self.conv4_3(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.pool4(h)\n",
    "        if debug: print(h.data.shape)\n",
    "\n",
    "        h = self.relu5_1(self.conv5_1(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.relu5_2(self.conv5_2(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.relu5_3(self.conv5_3(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.pool5(h)\n",
    "        if debug: print(h.data.shape)\n",
    "\n",
    "        h = self.relu6(self.fc6(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.drop6(h)\n",
    "        if debug: print(h.data.shape)\n",
    "\n",
    "        h = self.relu7(self.fc7(h))\n",
    "        if debug: print(h.data.shape)\n",
    "        h = self.drop7(h)\n",
    "        if debug: print(h.data.shape)\n",
    "\n",
    "        h = self.score_fr(h)\n",
    "        if debug: print(h.data.shape)\n",
    "\n",
    "        h = self.upscore(h)\n",
    "        if debug: print(h.data.shape)\n",
    "        h = h[:, :, 19:19 + x.size()[2], 19:19 + x.size()[3]].contiguous()\n",
    "        if debug: print(h.data.shape)\n",
    "            \n",
    "        return h\n",
    "\n",
    "    def copy_params_from_vgg16(self, vgg16):\n",
    "        features = [\n",
    "            self.conv1_1, self.relu1_1,\n",
    "            self.conv1_2, self.relu1_2,\n",
    "            self.pool1,\n",
    "            self.conv2_1, self.relu2_1,\n",
    "            self.conv2_2, self.relu2_2,\n",
    "            self.pool2,\n",
    "            self.conv3_1, self.relu3_1,\n",
    "            self.conv3_2, self.relu3_2,\n",
    "            self.conv3_3, self.relu3_3,\n",
    "            self.pool3,\n",
    "            self.conv4_1, self.relu4_1,\n",
    "            self.conv4_2, self.relu4_2,\n",
    "            self.conv4_3, self.relu4_3,\n",
    "            self.pool4,\n",
    "            self.conv5_1, self.relu5_1,\n",
    "            self.conv5_2, self.relu5_2,\n",
    "            self.conv5_3, self.relu5_3,\n",
    "            self.pool5,\n",
    "        ]\n",
    "        for l1, l2 in zip(vgg16.features, features):\n",
    "            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data = l1.weight.data\n",
    "                l2.bias.data = l1.bias.data\n",
    "        for i, name in zip([0, 3], ['fc6', 'fc7']):\n",
    "            l1 = vgg16.classifier[i]\n",
    "            l2 = getattr(self, name)\n",
    "            l2.weight.data = l1.weight.data.view(l2.weight.size())\n",
    "            l2.bias.data = l1.bias.data.view(l2.bias.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/surgery.py\n",
    "def get_upsampling_weight(in_channels, out_channels, kernel_size):\n",
    "    \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:kernel_size, :kernel_size]\n",
    "    filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n",
    "                      dtype=np.float64)\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return torch.from_numpy(weight).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From VGG16 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import fcn #PIP INSTALL\n",
    "\n",
    "def VGG16(pretrained=False, folder='data/pretrained_models'):\n",
    "    model = torchvision.models.vgg16(pretrained=False)\n",
    "    if not pretrained:\n",
    "        return model\n",
    "    model_file = _get_vgg16_pretrained_model(folder)\n",
    "    \n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def _get_vgg16_pretrained_model(folder):\n",
    "    path_model = osp.join(os.getcwd(), folder, 'vgg16_from_caffe.pth')\n",
    "    return fcn.data.cached_download(\n",
    "                url='http://drive.google.com/uc?id=0B9P1L--7Wd2vLTJZMXpIRkVVRFk',\n",
    "                path=path_model,\n",
    "                md5='aa75b158f4181e7f6230029eb96c1b13',\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCN32s(n_class=21)\n",
    "if cuda:\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_loader=iter(train_loader)\n",
    "data, target = next(iter_loader)\n",
    "if cuda:\n",
    "    data = data.to('cuda')\n",
    "with torch.no_grad():\n",
    "    output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('input: ', data.shape)\n",
    "print('output: ', output.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter_loader)\n",
    "if cuda:\n",
    "    data = data.to('cuda')\n",
    "with torch.no_grad():\n",
    "    output = model(data, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter_loader)\n",
    "if cuda:\n",
    "    data = data.to('cuda')\n",
    "with torch.no_grad():\n",
    "    output = model(data, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter_loader)\n",
    "if cuda:\n",
    "    data = data.to('cuda')\n",
    "with torch.no_grad():\n",
    "    output = model(data, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume:\n",
    "    print('Loading checkpoint from: '+resume)\n",
    "    checkpoint = torch.load(resume)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "else:\n",
    "    vgg16 = VGG16(pretrained=True) # It takes a while\n",
    "    model.copy_params_from_vgg16(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(model, bias=False):\n",
    "    import torch.nn as nn\n",
    "    modules_skipped = (\n",
    "        nn.ReLU,\n",
    "        nn.MaxPool2d,\n",
    "        nn.Dropout2d,\n",
    "        nn.Sequential,\n",
    "        FCN32s,\n",
    "    )\n",
    "    for idx, m in enumerate(model.modules()):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            if bias:\n",
    "                yield m.bias\n",
    "            else:\n",
    "                yield m.weight\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            # weight is frozen because it is just a bilinear upsampling\n",
    "            if bias:\n",
    "                assert m.bias is None\n",
    "        elif isinstance(m, modules_skipped) or idx==0:\n",
    "            continue\n",
    "        else:\n",
    "            raise ValueError('Unexpected module: %s' % str(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD([\n",
    "            {'params': get_parameters(model, bias=False)},\n",
    "            {'params': get_parameters(model, bias=True), 'lr': cfg['lr'] * 2, 'weight_decay': 0},\n",
    "                        ],\n",
    "            lr=cfg['lr'],\n",
    "            momentum=cfg['momentum'],\n",
    "            weight_decay=cfg['weight_decay']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume:\n",
    "    optim.load_state_dict(checkpoint['optim_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import datetime\n",
    "from distutils.version import LooseVersion\n",
    "import math\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "\n",
    "import fcn\n",
    "import numpy as np\n",
    "import pytz\n",
    "import scipy.misc\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import utils\n",
    "import imageio\n",
    "\n",
    "def cross_entropy2d(input, target, weight=None, size_average=True):\n",
    "    # input: (n, c, h, w), target: (n, h, w)\n",
    "    n, c, h, w = input.size()\n",
    "    # log_p: (n, c, h, w)\n",
    "    log_p = F.log_softmax(input, dim=1)\n",
    "    # log_p: (n*h*w, c)\n",
    "    log_p = log_p.transpose(1, 2).transpose(2, 3).contiguous()\n",
    "    log_p = log_p[target.view(n, h, w, 1).repeat(1, 1, 1, c) >= 0]\n",
    "    log_p = log_p.view(-1, c)\n",
    "    # target: (n*h*w,)\n",
    "    mask = target >= 0\n",
    "    target = target[mask]\n",
    "    loss = F.nll_loss(log_p, target, weight=weight, size_average=False)\n",
    "    if size_average:\n",
    "        loss /= mask.data.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, cuda, model, optimizer,\n",
    "                 train_loader, val_loader, out, max_iter,\n",
    "                 size_average=False, interval_validate=None):\n",
    "        self.cuda = cuda\n",
    "\n",
    "        self.model = model\n",
    "        self.optim = optimizer\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        self.timestamp_start = \\\n",
    "            datetime.datetime.now(pytz.timezone('America/Bogota'))\n",
    "        self.size_average = size_average\n",
    "\n",
    "        if interval_validate is None:\n",
    "            self.interval_validate = len(self.train_loader)\n",
    "        else:\n",
    "            self.interval_validate = interval_validate\n",
    "\n",
    "        self.out = out\n",
    "        if not osp.exists(self.out):\n",
    "            os.makedirs(self.out)\n",
    "\n",
    "        self.log_headers = [\n",
    "            'epoch',\n",
    "            'iteration',\n",
    "            'train/loss',\n",
    "            'train/acc',\n",
    "            'train/acc_cls',\n",
    "            'train/mean_iu',\n",
    "            'train/fwavacc',\n",
    "            'valid/loss',\n",
    "            'valid/acc',\n",
    "            'valid/acc_cls',\n",
    "            'valid/mean_iu',\n",
    "            'valid/fwavacc',\n",
    "            'elapsed_time',\n",
    "        ]\n",
    "        if not osp.exists(osp.join(self.out, 'log.csv')):\n",
    "            with open(osp.join(self.out, 'log.csv'), 'w') as f:\n",
    "                f.write(','.join(self.log_headers) + '\\n')\n",
    "\n",
    "        self.epoch = 0\n",
    "        self.iteration = 0\n",
    "        self.max_iter = max_iter\n",
    "        self.best_mean_iu = 0\n",
    "\n",
    "    def validate(self):\n",
    "        training = self.model.training\n",
    "        self.model.eval()\n",
    "\n",
    "        n_class = len(self.val_loader.dataset.class_names)\n",
    "\n",
    "        val_loss = 0\n",
    "        visualizations = []\n",
    "        label_trues, label_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in tqdm.tqdm(\n",
    "                    enumerate(self.val_loader), total=len(self.val_loader),\n",
    "                    desc='Valid iteration=%d' % self.iteration, ncols=80,\n",
    "                    leave=False):\n",
    "                if self.cuda:\n",
    "                    data, target = data.to('cuda'), target.to('cuda')\n",
    "                score = self.model(data)\n",
    "\n",
    "                loss = cross_entropy2d(score, target,\n",
    "                                       size_average=self.size_average)\n",
    "                if np.isnan(float(loss.item())):\n",
    "                    raise ValueError('loss is nan while validating')\n",
    "                val_loss += float(loss.item()) / len(data)\n",
    "\n",
    "                imgs = data.data.cpu()\n",
    "                lbl_pred = score.data.max(1)[1].cpu().numpy()[:, :, :]\n",
    "                lbl_true = target.data.cpu()\n",
    "                for img, lt, lp in zip(imgs, lbl_true, lbl_pred):\n",
    "                    img, lt = self.val_loader.dataset.untransform(img, lt)\n",
    "                    label_trues.append(lt)\n",
    "                    label_preds.append(lp)\n",
    "                    if len(visualizations) < 9:\n",
    "                        viz = fcn.utils.visualize_segmentation(\n",
    "                            lbl_pred=lp, lbl_true=lt, img=img, n_class=n_class)\n",
    "                        visualizations.append(viz)\n",
    "        metrics = utils.label_accuracy_score(\n",
    "            label_trues, label_preds, n_class)\n",
    "\n",
    "        out = osp.join(self.out, 'visualization_viz')\n",
    "        if not osp.exists(out):\n",
    "            os.makedirs(out)\n",
    "        out_file = osp.join(out, 'iter%012d.jpg' % self.iteration)\n",
    "        img_ = fcn.utils.get_tile_image(visualizations)\n",
    "        #scipy.misc.imsave(out_file, img_)\n",
    "        imageio.imwrite(out_file, img_)\n",
    "        plt.imshow(imageio.imread(out_file))\n",
    "        plt.show()\n",
    "\n",
    "        val_loss /= len(self.val_loader)\n",
    "\n",
    "        with open(osp.join(self.out, 'log.csv'), 'a') as f:\n",
    "            elapsed_time = (\n",
    "                datetime.datetime.now(pytz.timezone('America/Bogota')) -\n",
    "                self.timestamp_start).total_seconds()\n",
    "            log = [self.epoch, self.iteration] + [''] * 5 + \\\n",
    "                  [val_loss] + list(metrics) + [elapsed_time]\n",
    "            log = map(str, log)\n",
    "            f.write(','.join(log) + '\\n')\n",
    "\n",
    "        mean_iu = metrics[2]\n",
    "        is_best = mean_iu > self.best_mean_iu\n",
    "        if is_best:\n",
    "            self.best_mean_iu = mean_iu\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'iteration': self.iteration,\n",
    "            'arch': self.model.__class__.__name__,\n",
    "            'optim_state_dict': self.optim.state_dict(),\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'best_mean_iu': self.best_mean_iu,\n",
    "        }, osp.join(self.out, 'checkpoint.pth.tar'))\n",
    "        if is_best:\n",
    "            shutil.copy(osp.join(self.out, 'checkpoint.pth.tar'),\n",
    "                        osp.join(self.out, 'model_best.pth.tar'))\n",
    "\n",
    "        if training:\n",
    "            self.model.train()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "\n",
    "        n_class = len(self.train_loader.dataset.class_names)\n",
    "\n",
    "        for batch_idx, (data, target) in tqdm.tqdm(\n",
    "                enumerate(self.train_loader), total=len(self.train_loader),\n",
    "                desc='Train epoch=%d' % self.epoch, ncols=80, leave=False):\n",
    "            iteration = batch_idx + self.epoch * len(self.train_loader)\n",
    "            if self.iteration != 0 and (iteration - 1) != self.iteration:\n",
    "                continue  # for resuming\n",
    "            self.iteration = iteration\n",
    "\n",
    "            if self.iteration % self.interval_validate == 0:\n",
    "                self.validate()\n",
    "\n",
    "            assert self.model.training\n",
    "\n",
    "            if self.cuda:\n",
    "                data, target = data.to('cuda'), target.to('cuda')\n",
    "            self.optim.zero_grad()\n",
    "            score = self.model(data)\n",
    "\n",
    "            loss = cross_entropy2d(score, target,\n",
    "                                   size_average=self.size_average)\n",
    "            loss /= len(data)\n",
    "            if np.isnan(float(loss.item())):\n",
    "                raise ValueError('loss is nan while training')\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "            metrics = []\n",
    "            lbl_pred = score.data.max(1)[1].cpu().numpy()[:, :, :]\n",
    "            lbl_true = target.data.cpu().numpy()\n",
    "            acc, acc_cls, mean_iu, fwavacc = \\\n",
    "                utils.label_accuracy_score(\n",
    "                    lbl_true, lbl_pred, n_class=n_class)\n",
    "            metrics.append((acc, acc_cls, mean_iu, fwavacc))\n",
    "            metrics = np.mean(metrics, axis=0)\n",
    "\n",
    "            with open(osp.join(self.out, 'log.csv'), 'a') as f:\n",
    "                elapsed_time = (\n",
    "                    datetime.datetime.now(pytz.timezone('America/Bogota')) -\n",
    "                    self.timestamp_start).total_seconds()\n",
    "                log = [self.epoch, self.iteration] + [loss.item()] + \\\n",
    "                    metrics.tolist() + [''] * 5 + [elapsed_time]\n",
    "                log = map(str, log)\n",
    "                f.write(','.join(log) + '\\n')\n",
    "\n",
    "            if self.iteration >= self.max_iter:\n",
    "                break\n",
    "\n",
    "    def train(self):\n",
    "        max_epoch = int(math.ceil(1. * self.max_iter / len(self.train_loader)))\n",
    "        for epoch in tqdm.trange(self.epoch, max_epoch,\n",
    "                                 desc='Train', ncols=80):\n",
    "            self.epoch = epoch\n",
    "            self.train_epoch()\n",
    "            if self.iteration >= self.max_iter:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        cuda=cuda,\n",
    "        model=model,\n",
    "        optimizer=optim,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        out=out,\n",
    "        max_iter=cfg['max_iteration'],\n",
    "        interval_validate=cfg.get('interval_validate', len(train_loader)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "data, target = next(iter_loader)\n",
    "if cuda:\n",
    "    model.to('cuda')\n",
    "    data, target = data.to('cuda'), target.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.get('interval_validate', len(train_loader))) #Validate every 4000 iterations\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "start_iteration = 0\n",
    "if resume:\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    start_iteration = checkpoint['iteration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.epoch = start_epoch\n",
    "trainer.iteration = start_iteration\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
